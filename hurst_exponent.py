# -*- coding: utf-8 -*-
"""hust_exponent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QM6l0kzKlVQAgI0ww8SVWHmcM0fI6ytb
"""

# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import yfinance as yf
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss
from scipy.stats import linregress
import warnings
# Suppress warnings
warnings.filterwarnings("ignore", category=FutureWarning)
from statsmodels.tools.sm_exceptions import InterpolationWarning
warnings.simplefilter('ignore', InterpolationWarning)
from google.colab import files

# Download stock data from Nepse
def import_stock_data(file_path):
  data = pd.read_csv(file_path)
  data = data.set_index('Date')
  #convert index column to datetime format
  data.index = pd.to_datetime(data.index)
  # Remove commas and convert to float
  data['Volume'] = data['Volume'].str.replace(',', '').astype(float)
  return data

def hurst_exponent(ts):
    """
    Calculate the Hurst exponent using the Rescaled Range (R/S) method.

    Parameters:
        ts (array-like): The time series to analyze.

    Returns:
        float: The Hurst exponent.
    """
    ts = np.asarray(ts)
    n = len(ts)
    min_chunk_size = 10  # Minimum number of observations in a chunk
    chunk_sizes = [2**i for i in range(4, int(np.log2(n)))]  # Chunk sizes as powers of 2

    rs_values = []
    chunk_sizes_used = []

    for chunk_size in chunk_sizes:
        if chunk_size > n or chunk_size < min_chunk_size:
            continue

        num_chunks = n // chunk_size
        chunks = np.array_split(ts[:num_chunks * chunk_size], num_chunks)

        rs_chunks = []

        for chunk in chunks:
            mean_chunk = np.mean(chunk)
            mean_centered = chunk - mean_chunk
            cumulative_deviation = np.cumsum(mean_centered)
            r = np.max(cumulative_deviation) - np.min(cumulative_deviation)
            s = np.std(chunk, ddof=1)

            if s == 0:
                continue  # Skip chunks with zero standard deviation

            rs = r / s
            rs_chunks.append(rs)

        if rs_chunks:  # Only proceed if there are valid R/S values
            mean_rs = np.mean(rs_chunks)
            rs_values.append(mean_rs)
            chunk_sizes_used.append(chunk_size)

    if not rs_values:  # Handle case where no valid R/S values were computed
        raise ValueError("No valid R/S values computed. Check the input time series.")

    log_chunk_sizes = np.log(chunk_sizes_used)
    log_rs_values = np.log(rs_values)

    slope, _, _, _, _ = linregress(log_chunk_sizes, log_rs_values)

    hurst = slope
    return hurst

# Function to check trending behavior using Hurst exponent
def analyze_behavior(ts, threshold=0.5):
    """
    Classify a time series based on its Hurst exponent.

    Parameters:
        ts (array-like): The time series to analyze.
        threshold (float): Threshold for considering (default: 0.5).

    Returns:
        str: Classification of the time series:
            - "Mean Reverting" if H < 0.5
            - "Geometric Brownian Motion" if H is equal to 0.5
            - "Trending" if H > 0.5
    """
    hurst = hurst_exponent(ts)

    if hurst < threshold:
        classification = "Mean Reverting"
    elif hurst == threshold:
        classification = "A geometric Brownian motion"
    else:
        classification = "Trending"
    return classification, hurst

# Analyze securities for mean reversion and trending behavior
def analyze_securities(file_path):
    data = import_stock_data(file_path)

    results = []

    # Check for trending behavior
    classification, hurst = analyze_behavior(data['Close'])

    # Append results
    results.append({
            'Hurst Exponent': hurst,
            'Classification': classification
        })

    return pd.DataFrame(results), data

def kpss_test(data, regression="c", nlags="auto"):
    """
    Perform the KPSS test on the given data and return the results as a Pandas DataFrame.

    Parameters:
        data (pd.Series): The time series data (typically closing prices).
        regression (str): Type of regression for KPSS test ('c' for constant, 'ct' for constant and trend).
        nlags (str or int): The number of lags to include in the test. Set to 'auto' to automatically choose.

    Returns:
        pd.DataFrame: DataFrame containing the KPSS test results, including test statistic, p-value, lags, and critical values.
    """
    # Perform the KPSS test
    kpss_result = kpss(data, regression=regression, nlags=nlags)

    # Create a dictionary to store the results
    kpss_results_dict = {
        "Test Statistic": kpss_result[0],
        "p-value": kpss_result[1],
        "Lags Used": kpss_result[2],
    }

    # Add critical values to the dictionary
    for key, value in kpss_result[3].items():
        kpss_results_dict[f"Critical Value ({key})"] = value

    # Convert the results to a Pandas DataFrame
    kpss_results_df = pd.DataFrame(kpss_results_dict, index=[0])

    return kpss_results_df

def adf_test(data):
    """
    Perform the ADF test on the given data and return the results as a Pandas DataFrame.

    Parameters:
        data (pd.Series): The time series data (typically closing prices).

    Returns:
        pd.DataFrame: DataFrame containing the ADF test results, including test statistic, p-value, lags, and critical values.
    """
    # Perform the ADF test
    dftest = adfuller(data, autolag="AIC")

    # Create a dictionary to store the results
    adf_results_dict = {
        "Test Statistic": dftest[0],
        "p-value": dftest[1],
        "Lags Used": dftest[2],
    }

    # Add critical values to the dictionary
    for key, value in dftest[4].items():
        adf_results_dict[f"Critical Value ({key})"] = value

    # Convert the results to a Pandas DataFrame
    adf_results_df = pd.DataFrame(adf_results_dict, index=[0])

    return adf_results_df

# Function to analyze multiple tickers using yfinance
def analyze_multiple_tickers(tickers, start_date, end_date):
    all_results = []

    for ticker in tickers:   
            # Download stock data
            data = yf.download(ticker, start=start_date, end=end_date)

            # Perform Hurst analysis
            classification, hurst = analyze_behavior(data['Close'])
            
            # Perform KPSS and ADF tests
            kpss_results = kpss_test(data['Close'])
            adf_results = adf_test(data['Close'])
            
            # Interpret results
            adf_stationary = adf_results.iloc[:,1].values < adf_results.iloc[:,4].values 
            kpss_stationary = kpss_results.iloc[:,1].values < kpss_results.iloc[:,4].values

            if not adf_stationary and not kpss_stationary:
                conclusion = "The series is not stationary."
            elif adf_stationary and kpss_stationary:
                conclusion = "The series is stationary."
            elif not adf_stationary and kpss_stationary:
                conclusion = "The series is trend stationary. Detrending is needed."
            elif adf_stationary and not kpss_stationary:
                conclusion = "The series is difference stationary. Differencing is needed."
            
            # Combine all results
            ticker_results = {
                "Ticker": ticker,
                "Hurst Exponent": hurst,
                "Classification": classification,
                "KPSS Test Statistic": kpss_results.loc[0, "Test Statistic"],
                "KPSS p-value": kpss_results.loc[0, "p-value"],
                "KPSS Critical Value (5%)": kpss_results.loc[0, "Critical Value (5%)"],
                "ADF Test Statistic": adf_results.loc[0, "Test Statistic"],
                "ADF p-value": adf_results.loc[0, "p-value"],
                "ADF Critical Value (5%)": adf_results.loc[0, "Critical Value (5%)"],
                "Conclusion": conclusion,
            }
            all_results.append(ticker_results)

    return pd.DataFrame(all_results)